{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:19:30.205275412Z",
     "start_time": "2024-10-07T19:19:30.195329148Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Simple Vertically Partitioned Split Neural Network\n",
    "\n",
    "- <b>Alice</b>\n",
    "    - Has model Segment 1\n",
    "    - Has the handwritten Images\n",
    "- <b>Bob</b>\n",
    "    - Has model Segment 2\n",
    "    - Has the image Labels\n",
    "    \n",
    "Based on [SplitNN - Tutorial 3](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/advanced/split_neural_network/Tutorial%203%20-%20Folded%20Split%20Neural%20Network.ipynb) from Adam J Hall - Twitter: [@AJH4LL](https://twitter.com/AJH4LL) 路 GitHub:  [@H4LL](https://github.com/H4LL)\n",
    "\n",
    "Authors:\n",
    "- Pavlos Papadopoulos 路 GitHub:  [@pavlos-p](https://github.com/pavlos-p)\n",
    "- Tom Titcombe 路 GitHub:  [@TTitcombe](https://github.com/TTitcombe)\n",
    "- Robert Sandmann 路 GitHub: [@rsandmann](https://github.com/rsandmann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitNN:\n",
    "    def __init__(self, models, optimizers):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "\n",
    "        self.data = []\n",
    "        self.remote_tensors = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = []\n",
    "        remote_tensors = []\n",
    "\n",
    "        data.append(self.models[0](x))\n",
    "\n",
    "        if data[-1].location == self.models[1].location:\n",
    "            remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "        else:\n",
    "            remote_tensors.append(\n",
    "                data[-1].detach().move(self.models[1].location).requires_grad_()\n",
    "            )\n",
    "\n",
    "        i = 1\n",
    "        while i < (len(models) - 1):\n",
    "            data.append(self.models[i](remote_tensors[-1]))\n",
    "\n",
    "            if data[-1].location == self.models[i + 1].location:\n",
    "                remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "            else:\n",
    "                remote_tensors.append(\n",
    "                    data[-1].detach().move(self.models[i + 1].location).requires_grad_()\n",
    "                )\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        data.append(self.models[i](remote_tensors[-1]))\n",
    "\n",
    "        self.data = data\n",
    "        self.remote_tensors = remote_tensors\n",
    "\n",
    "        return data[-1]\n",
    "\n",
    "    def backward(self):\n",
    "        for i in range(len(models) - 2, -1, -1):\n",
    "            if self.remote_tensors[i].location == self.data[i].location:\n",
    "                grads = self.remote_tensors[i].grad.copy()\n",
    "            else:\n",
    "                grads = self.remote_tensors[i].grad.copy().move(self.data[i].location)\n",
    "    \n",
    "            self.data[i].backward(grads)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m      2\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets, transforms\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn, optim\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "from src.dataloader import VerticalDataLoader\n",
    "from src.psi.util import Client, Server\n",
    "from src.utils import add_ids\n",
    "\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "data = add_ids(MNIST)(\".\", download=True, transform=ToTensor())  # add_ids adds unique IDs to data points\n",
    "\n",
    "# Batch data\n",
    "dataloader = VerticalDataLoader(data, batch_size=128) # partition_dataset uses by default \"remove_data=True, keep_order=False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the datasets are unordered\n",
    "In MNIST, we have 2 datasets (the images and the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need matplotlib library to plot the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first 10 entries of the labels and the dataset\n",
    "figure = plt.figure()\n",
    "num_of_entries = 10\n",
    "for index in range(1, num_of_entries + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(dataloader.dataloader1.dataset.data[index].numpy().squeeze(), cmap='gray_r')\n",
    "    print(dataloader.dataloader2.dataset[index][0], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement PSI and order the datasets accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute private set intersection\n",
    "client_items = dataloader.dataloader1.dataset.get_ids()\n",
    "server_items = dataloader.dataloader2.dataset.get_ids()\n",
    "\n",
    "client = Client(client_items)\n",
    "server = Server(server_items)\n",
    "\n",
    "setup, response = server.process_request(client.request, len(client_items))\n",
    "intersection = client.compute_intersection(setup, response)\n",
    "\n",
    "# Order data\n",
    "dataloader.drop_non_intersecting(intersection)\n",
    "dataloader.sort_by_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check again if the datasets are ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need matplotlib library to plot the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first 10 entries of the labels and the dataset\n",
    "figure = plt.figure()\n",
    "num_of_entries = 10\n",
    "for index in range(1, num_of_entries + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(dataloader.dataloader1.dataset.data[index].numpy().squeeze(), cmap='gray_r')\n",
    "    print(dataloader.dataloader2.dataset[index][0], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Define our model segments\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 640]\n",
    "output_size = 10\n",
    "\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "    nn.Sequential(nn.Linear(hidden_sizes[1], output_size), nn.LogSoftmax(dim=1)),\n",
    "]\n",
    "\n",
    "# Create optimisers for each segment and link to them\n",
    "optimizers = [\n",
    "    optim.SGD(model.parameters(), lr=0.03,)\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "\n",
    "# Send Model Segments to model locations\n",
    "model_locations = [alice, bob]\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)\n",
    "\n",
    "#Instantiate a SpliNN class with our distributed segments and their respective optimizers\n",
    "splitNN = SplitNN(models, optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, target, splitNN):\n",
    "    \n",
    "    #1) Zero our grads\n",
    "    splitNN.zero_grads()\n",
    "    \n",
    "    #2) Make a prediction\n",
    "    pred = splitNN.forward(x)\n",
    "    \n",
    "    #3) Figure out how much we missed by\n",
    "    criterion = nn.NLLLoss()\n",
    "    loss = criterion(pred, target)\n",
    "    \n",
    "    #4) Backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    \n",
    "    #5) Feed Gradients backward through the nework\n",
    "    splitNN.backward()\n",
    "    \n",
    "    #6) Change the weights\n",
    "    splitNN.step()\n",
    "    \n",
    "    return loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for (data, ids1), (labels, ids2) in dataloader:\n",
    "        # Train a model\n",
    "        data = data.send(models[0].location)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        labels = labels.send(models[-1].location)\n",
    "\n",
    "        # Call model\n",
    "        loss, preds = train(data, labels, splitNN)\n",
    "\n",
    "        # Collect statistics\n",
    "        running_loss += loss.get()\n",
    "        correct_preds += preds.max(1)[1].eq(labels).sum().get().item()\n",
    "        total_preds += preds.get().size(0)\n",
    "\n",
    "    print(f\"Epoch {i} - Training loss: {running_loss/len(dataloader):.3f} - Accuracy: {100*correct_preds/total_preds:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels pointing to: \", labels)\n",
    "print(\"Images pointing to: \", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
